{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# jupyter notebook cell 너비 조절\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Project for ML/DL : [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이전 단계에서 전처리를 완료한 Data를 Machine Learning 알고리즘을 통해 학습하는 것을 목표로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "* Root-Mean-Squared-Error(RMSE)\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{|\\hat{R}|} \\sum_{\\hat{r}_{ui} \\in \\hat{R}}(r_{ui} - \\hat{r}_{ui})^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1458, 255)\n",
      "(1459, 255)\n",
      "(1458, 234)\n",
      "(1459, 234)\n",
      "(1458,)\n"
     ]
    }
   ],
   "source": [
    "dfX_all = joblib.load('dfX_all.pkl')\n",
    "dfX_test_all = joblib.load('dfX_test_all.pkl')\n",
    "\n",
    "dfX_removed = joblib.load('dfX_removed.pkl')\n",
    "dfX_test_removed = joblib.load('dfX_test_removed.pkl')\n",
    "\n",
    "dfy = joblib.load('dfy.pkl')\n",
    "\n",
    "print(dfX_all.shape)\n",
    "print(dfX_test_all.shape)\n",
    "print(dfX_removed.shape)\n",
    "print(dfX_test_removed.shape)\n",
    "print(dfy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Index\n",
    "## Step 1 : Set criteria(Linear Regression)\n",
    "## Step 2 : Single Algorithm learning\n",
    "  * Ridge 회귀\n",
    "  * Lasso 회귀\n",
    "  * ElasticNet 회귀\n",
    "  * Gradient Boosting Regression\n",
    "  * Support Vector Regression\n",
    "  * XGBoost Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Set Criteria t : Linear Regression \n",
    "* 가장 간단한 회귀 알고리즘은 Linear Regression\n",
    "* Linear Regression은 복잡도를 제어할 수 없는 간단한 모형이기 때문에 Linear Regression으로 구한 값보다 좋으면 양호한 것으로 판단한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "linear_score_all = np.sqrt(-cross_val_score(linear_model, dfX_all, dfy, scoring=\"neg_mean_squared_error\", cv=cv))\n",
    "linear_score_part = np.sqrt(-cross_val_score(linear_model, dfX_removed, dfy, scoring=\"neg_mean_squared_error\", cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression(all) RMSE : 0.12620 (0.01200)\n",
      "Linear Regression(removed) RMSE : 0.12353 (0.01218)\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear Regression(all) RMSE : {:.5f} ({:.5f})\".format(linear_score_all.mean(), linear_score_all.std()))\n",
    "print(\"Linear Regression(removed) RMSE : {:.5f} ({:.5f})\".format(linear_score_part.mean(), linear_score_part.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 제출 점수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all = LinearRegression().fit(dfX_all, dfy)\n",
    "model_removed = LinearRegression().fit(dfX_removed, dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx = pd.DataFrame(np.array(dfX_test_all.reset_index()[\"index\"] + 1), columns=[\"Id\"])\n",
    "\n",
    "linear_pred_all = pd.DataFrame(np.exp(model_all.predict(dfX_test_all)), columns=[\"SalePrice\"])\n",
    "linear_pred_removed = pd.DataFrame(np.exp(model_removed.predict(dfX_test_removed)), columns=[\"SalePrice\"])\n",
    "\n",
    "linear_sub_all = pd.concat([df_idx, linear_pred_all], axis=1)\n",
    "linear_sub_removed = pd.concat([df_idx, linear_pred_removed], axis=1)\n",
    "\n",
    "linear_sub_all.to_csv('./linear_sub_all.csv', index=False)\n",
    "linear_sub_removed.to_csv('./linear_sub_removed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Score : submission 점수(validation과 오차)\n",
    "* linear_sub_all RMSE score : 0.13514 (0.00894)\n",
    "* linear_sub_removed RMSE score : 0.14042 (0.01689)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_all_sub_score = 0.13514\n",
    "linear_removed_sub_score = 0.14042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_RMSE(algo_name, score_all, score_removed, gs_all, gs_removed):\n",
    "    print(\"{}(all) RMSE : {:.5f} ({:.5f})\".format(algo_name, score_all.mean(), score_all.std()))\n",
    "    print(\"{}(removed) RMSE : {:.5f} ({:.5f})\".format(algo_name, score_removed.mean(), score_removed.std()), \"\\n\")\n",
    "\n",
    "\n",
    "    print(\"{} parameters(all) :\".format(algo_name), gs_all.best_estimator_.steps[0][1])\n",
    "    print(\"{} parameters(removed) :\".format(algo_name), gs_removed.best_estimator_.steps[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_sub_to_csv(gs_all, gs_removed, algo_name):\n",
    "    pred_all = pd.DataFrame(np.exp(gs_all.best_estimator_.steps[0][1].predict(dfX_test_all)), columns=[\"SalePrice\"])\n",
    "    pred_removed = pd.DataFrame(np.exp(gs_removed.best_estimator_.steps[0][1].predict(dfX_test_removed)), columns=[\"SalePrice\"])\n",
    "    \n",
    "    sub_all = pd.concat([df_idx, pred_all], axis=1)\n",
    "    sub_removed = pd.concat([df_idx, pred_removed], axis=1)\n",
    "    \n",
    "    sub_all.to_csv('./{}_sub_all.csv'.format(algo_name), index=False)\n",
    "    sub_removed.to_csv('./{}_sub_removed.csv'.format(algo_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Single Algorithm learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge 회귀 모형\n",
    "* 선형회귀 계수(weigt)에 대한 제약 조건을 추가함으로써 과최적화를 막는 정규화 선형회귀 방법 중 하나이다.\n",
    "* 가중치들의 제곱합을 최소화 하는 것을 추가적인 제약 조건으로 한다.\n",
    "$$ w = \\text{arg}\\min_w \\left(\\sum_{i=1}^N e_i^2 + \\lambda \\sum_{j_1}^M w_j^2 \\right)$$\n",
    "* $ \\lambda \\sum_{j_1}^M w_j^2 $ 이 추가된 규제항이 된다.\n",
    "* `alpha` : 하이퍼모수 $\\lambda$. 정규화 정도를 조절하며 크면 정규화 정도가 커지고 가중치의 값들이 작이진다. 0이 되면 일반적인 선형회귀모형이 된다.\n",
    "* `max_iter` : gradient solver의 최대 반복 횟수이다.\n",
    "* 규제항은 훈련하는 동안에만 비용 함수에 추가되며 성능을 평가하거나 예측할 때는 포함하지 않고 규제가 없는 성능 지표로 평가한다.\n",
    "* 단점은 모든 예측 변수를 중요도에 따라 가중값만 축소시킬 뿐, 0값을 부여하지 않기 때문에 불필요한 변수가 제거되지 않고 항상 남아 있게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_alpha = list(np.arange(0.5, 20, 0.5))\n",
    "ridge_iter = list(range(1000, 3001, 500))\n",
    "pipe_ridge = Pipeline([(\"ridge\", Ridge())])\n",
    "param_grid = [{\"ridge__alpha\" : ridge_alpha}, {\"ridge__max_iter\" : ridge_iter}]\n",
    "\n",
    "ridge_gs1 = GridSearchCV(estimator=pipe_ridge, param_grid=param_grid, scoring=\"neg_mean_squared_error\", cv=cv, n_jobs=-1)\n",
    "ridge_gs2 = GridSearchCV(estimator=pipe_ridge, param_grid=param_grid, scoring=\"neg_mean_squared_error\", cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "r_gs_all = ridge_gs1.fit(dfX_all, dfy)\n",
    "r_gs_removed = ridge_gs2.fit(dfX_removed, dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 553 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge_score_all = np.sqrt(-cross_val_score(r_gs_all.best_estimator_, dfX_all, dfy, scoring=\"neg_mean_squared_error\", cv=cv))\n",
    "ridge_score_removed = np.sqrt(-cross_val_score(r_gs_removed.best_estimator_, dfX_removed, dfy, scoring=\"neg_mean_squared_error\", cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(all) RMSE : 0.11286 (0.01269)\n",
      "Ridge(removed) RMSE : 0.11593 (0.01321) \n",
      "\n",
      "Ridge parameters(all) : Ridge(alpha=19.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Ridge parameters(removed) : Ridge(alpha=11.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "check_RMSE(\"Ridge\", ridge_score_all, ridge_score_removed, r_gs_all, r_gs_removed)\n",
    "store_sub_to_csv(r_gs_all, r_gs_removed, \"ridge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Score : submission 점수(validation과 오차)\n",
    "* ridge_sub_all RMSE score : 0.11983 (0.00697)\n",
    "* ridge_sub_removed RMSE score : 0.12751 (0.01158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all_sub_score = 0.11983\n",
    "r_removed_sub_score = 0.12751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso 회귀 모형\n",
    "* 선형회귀 계수(weigt)에 대한 제약 조건을 추가함으로써 과최적화를 막는 정규화 선형회귀 방법 중 하나이다.\n",
    "* 가중치들의 절대값의 합을 최소화 하는 것을 추가적인 제약 조건으로 한다.\n",
    "\n",
    "$$ w = \\text{arg}\\min_w \\left(\\sum_{i=1}^N e_i^2 + \\lambda \\sum_{j=1}^M | w_j | \\right) $$\n",
    "* $ \\lambda \\sum_{j_1}^M | w_j | $ 이 추가된 규제항이 된다.\n",
    "* `alpha` : 하이퍼모수 $\\lambda$. 정규화 정도를 조절하며 크면 정규화 정도가 커지고 가중치의 값들이 작이진다. 0이 되면 일반적인 선형회귀모형이 된다.\n",
    "* Lasso의 중요한 특징은 덜 중요한 특성의 가중치를 0으로 만들어 완전히 제거하려고 한다는 점이다. 다시 말해 Lasso는 자동으로 특성 선택을 하고 희소 모델(sparse model)을 만든다. 즉, 0이 아닌 특성의 가중치가 적다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_alpha = list(np.arange(0.5, 20, 0.5))\n",
    "pipe_lasso = Pipeline([('lasso', Lasso())])\n",
    "param_grid = [{'lasso__alpha' : lasso_alpha}]\n",
    "\n",
    "lasso_gs1 = GridSearchCV(estimator=pipe_lasso, param_grid=param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "lasso_gs2 = GridSearchCV(estimator=pipe_lasso, param_grid=param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "l_gs_all = lasso_gs1.fit(dfX_all, dfy)\n",
    "l_gs_removed = lasso_gs2.fit(dfX_removed, dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lasso_score_all = np.sqrt(-cross_val_score(l_gs_all.best_estimator_, dfX_all, dfy, scoring=\"neg_mean_squared_error\", cv=cv))\n",
    "lasso_score_removed = np.sqrt(-cross_val_score(l_gs_removed.best_estimator_, dfX_removed, dfy, scoring=\"neg_mean_squared_error\", cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(all) RMSE : 0.16040 (0.01670)\n",
      "Lasso(removed) RMSE : 0.16104 (0.01657) \n",
      "\n",
      "Lasso parameters(all) : Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso parameters(removed) : Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "check_RMSE(\"Lasso\", lasso_score_all, lasso_score_removed, l_gs_all, l_gs_removed)\n",
    "store_sub_to_csv(l_gs_all, l_gs_removed, \"lasso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Score : submission 점수(validation과 오차)\n",
    "* lasso_sub_all RMSE score : 0.17650 (0.0161)\n",
    "* lasso_sub_removed RMSE score : 0.17847 (0.01743)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_all_sub_score = 0.17650\n",
    "l_removed_sub_score = 0.17847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet 회귀 모형\n",
    "* 선형회귀 계수(weigt)에 대한 제약 조건을 추가함으로써 과최적화를 막는 정규화 선형회귀 방법 중 하나이다\n",
    "* 가중치의 절대값의 합과 제곱합을 동시에 제약 조건으로 가지는 것으로, Ridge 회귀와 Lasso 회귀를 절충한 모델이다.\n",
    "$$ w = \\text{arg}\\min_w \\left( \\sum_{i=1}^N e_i^2 + \\lambda_1 \\sum_{j=1}^M | w_j | + \\lambda_2 \\sum_{j=1}^M w_j^2 \\right) $$\n",
    "* $\\lambda_1$과 $\\lambda_2$ 두 개의 하이퍼 모수를 가진다. 혼합 정도는 혼합 비율 r을 사용해 조절한다. r=0이면 Ridge 회귀와 같고 r=1이면 Lasso 회귀와 같다.\n",
    "* `alpha` : 정규화 정도를 조절하며 크면 정규화 정도가 커지고 가중치의 값들이 작이진다. 0이 되면 일반적인 선형회귀모형이 된다.\n",
    "* `l1_ratio` : $\\lambda_1$과 $\\lambda_2$ 의 혼합 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_alpha = list(np.arange(0.5, 20, 0.5))\n",
    "elastic_ratio = list(np.arange(0, 1, 0.1))\n",
    "elastic_iter = list(range(1000, 3001, 500))\n",
    "\n",
    "pipe_elastic = Pipeline([('elastic', ElasticNet())])\n",
    "param_grid = [{'elastic__alpha' : elastic_alpha}, {'elastic__l1_ratio' : elastic_ratio}, {'elastic__max_iter' : elastic_iter}]\n",
    "\n",
    "elastic_gs1 = GridSearchCV(estimator=pipe_elastic, param_grid=param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "elastic_gs2 = GridSearchCV(estimator=pipe_elastic, param_grid=param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "elastic_gs_all = elastic_gs1.fit(dfX_all, dfy)\n",
    "elastic_gs_removed = elastic_gs2.fit(dfX_removed, dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "elastic_score_all = np.sqrt(-cross_val_score(elastic_gs_all.best_estimator_, dfX_all, dfy, scoring=\"neg_mean_squared_error\", cv=cv))\n",
    "elastic_score_removed = np.sqrt(-cross_val_score(elastic_gs_removed.best_estimator_, dfX_removed, dfy, scoring=\"neg_mean_squared_error\", cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(all) RMSE : 0.12943 (0.01566)\n",
      "ElasticNet(removed) RMSE : 0.13380 (0.01696) \n",
      "\n",
      "ElasticNet parameters(all) : ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.0,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "ElasticNet parameters(removed) : ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.0,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "check_RMSE(\"ElasticNet\", elastic_score_all, elastic_score_removed, elastic_gs_all, elastic_gs_removed)\n",
    "store_sub_to_csv(elastic_gs_all, elastic_gs_removed, \"elastic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Score : submission 점수(validation과 오차)\n",
    "* elastic_sub_all RMSE score : 0.14259 (0.01316)\n",
    "* elastic_sub_removed RMSE score : 0.15012 (0.01632)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_all_sub_score = 0.14259\n",
    "elastic_removed_sub_score = 0.15012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression\n",
    "* 약한 학습기(Weak Learner)를 결합하여 강한 학습기(Strong Learner)를 만드는 Boosting 방법 중 하나로 Gradien descent를 사용하여 최적의 파라미터를 찾는 Boosting 방법이다.\n",
    "* Ensemble에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가한다. \n",
    "* `loss` : 손실함수(loss function)를 지정하는 파라미터. 'ls'는 least squares로 최소자승법(residual의 제곱의 합을 최소화)을 의미. 'lad'는 least absolute deviation으로 오차의 절대값의 합계를 의미한다. 'huber'는 huber loss로 ls와 lad를 절충한 것이다. 일정한 범위를 정해서 그 안에 있으면 오차를 구하고, 그 밖에 있으면 오차의 절대값을 구하는 것이다.\n",
    "* `max_features` : 분할할 최적의 숫자를 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_loss = ['ls', 'lad', 'huber']\n",
    "gbr_lr = [0.01, 0.05, 0.1, 0.5]\n",
    "gbr_estimator = list(range(100, 3501, 500))\n",
    "gbr_depth = range(3, 21, 1)\n",
    "gbr_features = [\"auto\", \"sqrt\", \"log2\", None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gbr = Pipeline([('gbr', GradientBoostingRegressor())])\n",
    "gbr_param_grid = [{'gbr__loss' : gbr_loss}, {'gbr__learning_rate' : gbr_lr}, {'gbr__n_estimators' : gbr_estimator}, {'gbr__max_depth' : gbr_depth}\n",
    "             , {'gbr__max_features' : gbr_features}]\n",
    "\n",
    "gbr_gs1 = GridSearchCV(estimator=pipe_gbr, param_grid=gbr_param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "gbr_gs2 = GridSearchCV(estimator=pipe_gbr, param_grid=gbr_param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "gbr_gs_all = gbr_gs1.fit(dfX_all, dfy)\n",
    "gbr_gs_removed = gbr_gs2.fit(dfX_removed, dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbr_score_all = np.sqrt(-cross_val_score(gbr_gs_all.best_estimator_, dfX_all, dfy, scoring=\"neg_mean_squared_error\", cv=cv))\n",
    "gbr_score_removed = np.sqrt(-cross_val_score(gbr_gs_removed.best_estimator_, dfX_removed, dfy, scoring=\"neg_mean_squared_error\", cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBR(all) RMSE : 0.11839 (0.01267)\n",
      "GBR(removed) RMSE : 0.12345 (0.01288) \n",
      "\n",
      "GBR parameters(all) : GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=1600, n_iter_no_change=None, presort='auto',\n",
      "             random_state=None, subsample=1.0, tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "GBR parameters(removed) : GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=3100, n_iter_no_change=None, presort='auto',\n",
      "             random_state=None, subsample=1.0, tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "check_RMSE(\"GBR\", gbr_score_all, gbr_score_removed, gbr_gs_all, gbr_gs_removed)\n",
    "store_sub_to_csv(gbr_gs_all, gbr_gs_removed, \"gbr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gbr_gs_removed.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(gbr_gs_all, \"gbr_gs_all.pkl\")\n",
    "joblib.dump(gbr_gs_removed, \"gbr_gs_removed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gbr_sub_all RMSE score : 0.13461 (0.01622)\n",
    "* gbr_sub_removed RMSE score : 0.13569 (0.01224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_all_sub_score = 0.13461\n",
    "gbr_removed_sub_score = 0.13569"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression\n",
    "* SVM의 maximal margin과 같은 특징들을 유지하여 Regression을 사용할 수 있다.\n",
    "* SVM 회귀는 SVM 분류의 목표와는 반대로 한다. 일정한 마진 오류 안에서 두 클래스 간의 도로 폭이 가능한 최대가 되도록 하면서 제한된 마진 오류 안에서 가능한 많은 샘플이 들어가도록 학습한다.\n",
    "* `kernel` : 알고리즘에 사용할 kernel 유형을 지정한다. 'rbf'는 가우시안 방사 기저 함수(Radial Basis Function)을 의미하며 비선형 특성을 다루는 함수 중 하나이다. 여기에서는 'rbf'만 사용한다. (kernel 인수의 종류로는 'linear', 'poly', 'rbf','sigmoid'가 있다.\n",
    "* `gamma` : 하나의 훈련 샘플이 영향을 미치는 범위를 결정한다. 작은 값은 넓은 범위를 의미하며 큰 값은 영향을 미치는 범위가 제한적이다.\n",
    "* `C` : error의 규제 parameter이다. C를 줄이면 margin의 폭이 넓어지지만 margin 오류도 커진다. C가 커지면 margin이 좁아지지만 margin 오류가 적다.\n",
    "* `epsilon` : margin의 폭을 지정한다.(허용오차와 다르다.)\n",
    "* `tol` : 허용오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_gamma = ['auto_deprecated', 1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
    "svr_C = [0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "svr_epsilon = [0.0001, 0.0005,0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5]\n",
    "svr_tol = [0.0001, 0.001, 0.01, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svr = Pipeline([('svr', SVR())])\n",
    "svr_param_grid = [{'svr__gamma' : svr_gamma}, {'svr__C' : svr_C}, {'svr__epsilon' : svr_epsilon}, {'svr__tol' : svr_tol}]\n",
    "\n",
    "svr_gs1 = GridSearchCV(estimator=pipe_svr, param_grid=svr_param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "svr_gs2 = GridSearchCV(estimator=pipe_svr, param_grid=svr_param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "svr_gs_all = svr_gs1.fit(dfX_all, dfy)\n",
    "svr_gs_removed = svr_gs2.fit(dfX_removed, dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svr_score_all = np.sqrt(-cross_val_score(svr_gs_all.best_estimator_, dfX_all, dfy, scoring=\"neg_mean_squared_error\", cv=cv))\n",
    "svr_score_removed = np.sqrt(-cross_val_score(svr_gs_removed.best_estimator_, dfX_removed, dfy, scoring=\"neg_mean_squared_error\", cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(all) RMSE : 0.39571 (0.02713)\n",
      "SVR(removed) RMSE : 0.39548 (0.02703) \n",
      "\n",
      "SVR parameters(all) : SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0001,\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "SVR parameters(removed) : SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0001,\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "check_RMSE(\"SVR\", svr_score_all, svr_score_removed, svr_gs_all, svr_gs_removed)\n",
    "store_sub_to_csv(svr_gs_all, svr_gs_removed, \"svr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* svr_sub_all RMSE score : 0.41716 (0.02145)\n",
    "* svr_sub_removed RMSE score : 0.41656 (0.02108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_all_sub_score = 0.41716\n",
    "svr_removed_sub_score = 0.41656"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svr_gs_removed.pkl']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svr_gs_all, \"svr_gs_all.pkl\")\n",
    "joblib.dump(svr_gs_removed, \"svr_gs_removed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression\n",
    "* XGBoost는 Extreme Gradient Boosting의 약자로 Gradient Boosting 알고리즘을 핵심으로 한다.\n",
    "* 병렬 처리를 사용하기 때문에 학습과 계산이 빠르고 Greedy-algorithm을 사용한 자동 가지치기로 과적합이 잘 발생하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_depth = list(range(3, 21))\n",
    "xgb_lr = [0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "xgb_estimators = list(range(100, 3501, 100))\n",
    "xgb_bytree = [0.6, 0.7, 0.8, 0.9, 1]\n",
    "xgb_child_weight = list(np.arange(0, 1.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_xgb = Pipeline([('xgb', XGBRegressor())])\n",
    "xgb_param_grid = [{'xgb__max_depth' : xgb_depth}, {'xgb__learning_rate' : xgb_lr}, {'xgb__n_estimators' : xgb_estimators}, \n",
    "                  {'xgb__colsample_bytree' : xgb_bytree}, {'xgb__min_child_weight' : xgb_child_weight}]\n",
    "xgb_gs1 = GridSearchCV(estimator=pipe_xgb, param_grid=xgb_param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "xgb_gs2 = GridSearchCV(estimator=pipe_xgb, param_grid=xgb_param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:38:50] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[01:37:03] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Wall time: 4h 33min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "xgb_gs_all = xgb_gs1.fit(dfX_all, dfy)\n",
    "xgb_gs_removed = xgb_gs2.fit(dfX_removed, dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:02:29] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:02:48] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:03:06] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:03:23] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:03:40] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:04:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:04:12] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:04:22] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:04:32] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:04:45] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_score_all = np.sqrt(-cross_val_score(xgb_gs_all.best_estimator_, dfX_all, dfy, scoring=\"neg_mean_squared_error\", cv=KFold(5, shuffle=True, random_state=0)))\n",
    "xgb_score_removed = np.sqrt(-cross_val_score(xgb_gs_removed.best_estimator_, dfX_removed, dfy, scoring=\"neg_mean_squared_error\", cv=KFold(5, shuffle=True, random_state=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB(all) RMSE : 0.11742 (0.00971)\n",
      "XGB(removed) RMSE : 0.12090 (0.01061) \n",
      "\n",
      "XGB parameters(all) : XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
      "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=None, subsample=1, verbosity=1)\n",
      "XGB parameters(removed) : XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=700,\n",
      "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "check_RMSE(\"XGB\", xgb_score_all, xgb_score_removed, xgb_gs_all, xgb_gs_removed)\n",
    "store_sub_to_csv(xgb_gs_all, xgb_gs_removed, \"xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_gs_removed.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_gs_all, \"xgb_gs_all.pkl\")\n",
    "joblib.dump(xgb_gs_removed, \"xgb_gs_removed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* xgb_sub_all RMSE score : 0.13020 (0.01278)\n",
    "* xgb_sub_removed RMSE score : 0.13660 (0.0157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_all_sub_score = 0.13020\n",
    "xgb_removed_sub_score = 0.13660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_gs_all = joblib.load(\"xgb_gs_all.pkl\")\n",
    "# xgb_gs_all = joblib.load(\"xgb_gs_removed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7개 알고리즘의 Submission Score 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_all_best = LinearRegression()\n",
    "linear_removed_best = LinearRegression()\n",
    "\n",
    "# r_all_best = r_gs_all.best_estimator_.steps[0][1]\n",
    "# r_removed_best = r_gs_removed.best_estimator_.steps[0][1]\n",
    "r_all_best = Ridge(alpha=19.5, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='auto', tol=0.001)\n",
    "r_removed_best = Ridge(alpha=11.0, copy_X=True, fit_intercept=True, max_iter=None,normalize=False, random_state=None, solver='auto', tol=0.001)\n",
    "\n",
    "# l_all_best = l_gs_all.best_estimator_.steps[0][1]\n",
    "# l_removed_best = l_gs_removed.best_estimator_.steps[0][1]\n",
    "l_all_best = Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None,\n",
    "                   selection='cyclic', tol=0.0001, warm_start=False)\n",
    "l_removed_best = Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None,\n",
    "                       selection='cyclic', tol=0.0001, warm_start=False)\n",
    "\n",
    "# elastic_all_best = elastic_gs_all.best_estimator_.steps[0][1]\n",
    "# elastic_removed_best = elastic_gs_removed.best_estimator_.steps[0][1]\n",
    "elastic_all_best = ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.0, max_iter=1000, normalize=False, positive=False, \n",
    "                              precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
    "elastic_removed_best = ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.0, max_iter=1000, normalize=False, positive=False, \n",
    "                                  precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
    "\n",
    "# gbr_all_best = gbr_gs_all.best_estimator_.steps[0][1]\n",
    "# gbr_removed_best = gbr_gs_removed.best_estimator_.steps[0][1]\n",
    "gbr_all_best = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None, learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
    "                                         max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,\n",
    "                                         min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=1600, n_iter_no_change=None, presort='auto',\n",
    "                                         random_state=None, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "gbr_removed_best = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None, learning_rate=0.1, loss='ls', max_depth=3, \n",
    "                                             max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                             min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=3100, \n",
    "                                             n_iter_no_change=None, presort='auto', random_state=None, subsample=1.0, tol=0.0001, \n",
    "                                             validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "\n",
    "# svr_all_best = svr_gs_all.best_estimator_.steps[0][1]\n",
    "# svr_removed_best = svr_gs_removed.best_estimator_.steps[0][1]\n",
    "svr_all_best = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0001, kernel='rbf', max_iter=-1, shrinking=True, \n",
    "                   tol=0.001, verbose=False)\n",
    "svr_removed_best = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0001, kernel='rbf', max_iter=-1, shrinking=True, \n",
    "                       tol=0.001, verbose=False)\n",
    "\n",
    "# xgb_all_best = xgb_gs_all.best_estimator_.steps[0][1]\n",
    "# xgb_removed_best = xgb_gs_removed.best_estimator_.steps[0][1]\n",
    "xgb_all_best = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, \n",
    "                            importance_type='gain', learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, \n",
    "                            n_estimators=1000, n_jobs=1, nthread=None, objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1, \n",
    "                            scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1)\n",
    "xgb_removed_best = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, \n",
    "                                importance_type='gain', learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, \n",
    "                                n_estimators=700, n_jobs=1, nthread=None, objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1, \n",
    "                                scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_names = [\"Linear Regression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"Gradient Boosting Regression\", \"Support Vector Regression\", \"XGBoost Regression\"]\n",
    "all_sub_scores = [linear_all_sub_score, r_all_sub_score, l_all_sub_score, elastic_all_sub_score, gbr_all_sub_score, svr_all_sub_score, xgb_all_sub_score]\n",
    "removed_sub_scores = [linear_removed_sub_score, r_removed_sub_score, l_removed_sub_score, elastic_removed_sub_score, gbr_removed_sub_score, \n",
    "                      svr_removed_sub_score, xgb_removed_sub_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Regression of All columns]\n",
      "Linear Regression (all) RMSE : 0.13514\n",
      "Ridge (all) RMSE : 0.11983\n",
      "Lasso (all) RMSE : 0.1765\n",
      "ElasticNet (all) RMSE : 0.14259\n",
      "Gradient Boosting Regression (all) RMSE : 0.13461\n",
      "Support Vector Regression (all) RMSE : 0.41716\n",
      "XGBoost Regression (all) RMSE : 0.1302\n",
      "\n",
      "\n",
      "[Regression of Removed columns]\n",
      "Linear Regression (all) RMSE : 0.14042\n",
      "Ridge (all) RMSE : 0.12751\n",
      "Lasso (all) RMSE : 0.17847\n",
      "ElasticNet (all) RMSE : 0.15012\n",
      "Gradient Boosting Regression (all) RMSE : 0.13569\n",
      "Support Vector Regression (all) RMSE : 0.41656\n",
      "XGBoost Regression (all) RMSE : 0.1366\n"
     ]
    }
   ],
   "source": [
    "print(\"[Regression of All columns]\")\n",
    "for i in range(len(algo_names)):\n",
    "    print(\"{} (all) RMSE : {}\".format(algo_names[i], all_sub_scores[i]))\n",
    "print(\"\\n\")\n",
    "    \n",
    "print(\"[Regression of Removed columns]\")\n",
    "for i in range(len(algo_names)):\n",
    "    print(\"{} (all) RMSE : {}\".format(algo_names[i], removed_sub_scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
